# Copyright 2022 Observational Health Data Sciences and Informatics
#
# This file is part of DiscoverySystemSimulator
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# library(dplyr)
# library(purrr)

#' Create setting for the discovery system
#'
#' @param alpha           The family-wise type 1 error rate we're aiming for.
#' @param useOracleForCv  Compute critical values on the actual sample sizes? If FALSE,
#'                        use expected sample sizes instead.
#'
#' @return
#' A settings object
#'
#' @export
createDiscoverySystemSettings <- function(alpha = 0.05,
                                          useOracleForCv = FALSE) {
  settings <- list()
  for (name in names(formals(createDiscoverySystemSettings))) {
    settings[[name]] <- get(name)
  }
  return(settings)
}


#' Run discovery system
#'
#' @param simulation              An object generated by [simulateDiscoverySystem()].
#' @param discoverySystemSettings An object generated by [createDiscoverySystemSettings()].
#'
#' @return
#' Returns a tibble with signal flags for each row in the simulation.
#'
#' @export
runDiscoverySystem <- function(simulation = simulateDiscoverySystem(),
                               discoverySystemSettings = createDiscoverySystemSettings()) {

  # Divide alpha over exposure-outcomes (Bonferroni)
  nExposureOutcomes <- simulation %>%
    distinct(.data$exposureOutcomeId) %>%
    count() %>%
    pull()

  alphaPerExposureOutcome <- discoverySystemSettings$alpha / nExposureOutcomes

  # Divide alpha over databases (Bonferroni)
  alphaPerDatabase <- simulation %>%
    group_by(.data$exposureOutcomeId) %>%
    distinct(.data$databaseId) %>%
    count(name = "nDatabases") %>%
    mutate(alpha = alphaPerExposureOutcome / .data$nDatabases)

  # Divide alpha over time at risks (Bonferroni)
  alphaPerTar <- simulation %>%
    group_by(.data$exposureOutcomeId, .data$databaseId) %>%
    distinct(.data$timeAtRiskId) %>%
    count(name = "nTars") %>%
    inner_join(alphaPerDatabase, by = "exposureOutcomeId") %>%
    mutate(alpha = .data$alpha / .data$nTars) %>%
    ungroup()

  # Divide alpha over methods (Bonferroni)
  alphaPerMethod <- simulation %>%
    group_by(.data$exposureOutcomeId, .data$databaseId, .data$timeAtRiskId) %>%
    distinct(.data$methodId) %>%
    count(name = "nMethods") %>%
    inner_join(alphaPerTar, by = c("exposureOutcomeId", "databaseId")) %>%
    mutate(alpha = .data$alpha / .data$nMethods) %>%
    ungroup()

  # Calibrated MaxSPRT
  signals <- performCalibratedMaxSprt(simulation, alphaPerMethod, discoverySystemSettings)
  return(signals)
}

performCalibratedMaxSprt <- function(simulation, alphaPerMethod, discoverySystemSettings) {
  if (discoverySystemSettings$useOracleForCv) {
    criticalValues <- computeCriticalValuesUsingOracle(simulation, alphaPerMethod)
  } else {
    criticalValues <- computeCriticalValuesUsingExpected(simulation, alphaPerMethod)
  }

  distributions <- fitSystematicErrorDistributions(simulation)

  data <- simulation %>%
    select(.data$methodId, .data$timeAtRiskId, .data$exposureOutcomeId, .data$databaseId, .data$lookId, .data$p, .data$llr, .data$profileIdx) %>%
    inner_join(criticalValues, by = c("methodId", "timeAtRiskId", "exposureOutcomeId", "databaseId")) %>%
    inner_join(distributions, by = c("methodId", "timeAtRiskId", "databaseId", "lookId")) %>%
    inner_join(alphaPerMethod, by = c("timeAtRiskId", "exposureOutcomeId", "databaseId"))

  message("Computing signals")
  # row <- split(data, 1:nrow(data))[[1]]
  computeSignals <- function(row) {
    profile <- attr(simulation, "profiles")[[row$profileIdx]]
    if (is.null(profile)) {
      calibratedLlr <- 0
    } else {
      null <- c(row$systematicErrorMean, row$systematicErrorSd)
      names(null) <- c("mean", "sd")
      class(null) <- "null"
      suppressMessages(
        calibratedLlr <- EmpiricalCalibration::calibrateLlr(null = null,
                                                            likelihoodApproximation = profile)
      )
    }
    row %>%
      mutate(signalMaxSprt = .data$llr > .data$cv,
             signalCalibratedMaxSprt = calibratedLlr > .data$cv,
             signalP = if_else(is.na(.data$p), FALSE, .data$p < row$alpha)) %>%
      return()
  }
  signals <- map_dfr(split(data, 1:nrow(data)), computeSignals)

  signals <- signals %>%
    select(.data$methodId,
           .data$timeAtRiskId,
           .data$exposureOutcomeId,
           .data$databaseId,
           .data$lookId,
           .data$signalMaxSprt,
           .data$signalCalibratedMaxSprt,
           .data$signalP) %>%
    dropNonStandardAttributes()
  return(signals)
}

dropNonStandardAttributes <- function(object) {
  standard <- names(attributes(tibble()))
  attributes(object) <- attributes(object)[standard]
  return(object)
}

fitSystematicErrorDistributions <- function(simulation) {
  # Using all simulated exposure-outcome pairs with null effect as negative controls:
  negativeControlIds <- getNegativeControlIds(attr(simulation, "simulationSettings"))

  message("Fitting systematic error distributions")
  # subset <- split(simulation, paste(simulation$databaseId, simulation$timeAtRiskId, simulation$methodId, simulation$lookId))[[1]]
  fitDistribution <- function(subset) {
    negativeControls <- subset %>%
      filter(.data$exposureOutcomeId %in% negativeControlIds)
    # Option: could also fit using normal approximation on per-control likelihood
    profiles <- attr(simulation, "profiles")[negativeControls$profileIdx]
    profiles <- profiles %>%
      discard(is.null)
    suppressMessages(
      distribution <- EmpiricalCalibration::fitNullNonNormalLl(profiles)
    )
    tibble(databaseId = subset$databaseId[1],
           timeAtRiskId = subset$timeAtRiskId[1],
           methodId = subset$methodId[1],
           lookId = subset$lookId[1],
           systematicErrorMean = distribution[1],
           systematicErrorSd = distribution[2]) %>%
      return()
  }
  distributions <- map_dfr(split(simulation, paste(simulation$databaseId,
                                                   simulation$timeAtRiskId,
                                                   simulation$methodId,
                                                   simulation$lookId)),
                           fitDistribution)
  return(distributions)
}

computeCriticalValuesUsingOracle <- function(simulation, alphaPerMethod) {
  sampleSizePerLook <- simulation %>%
    mutate(events = .data$targetEvents + .data$comparatorEvents,
           z = .data$comparatorTime / .data$targetTime) %>%
    select(.data$exposureOutcomeId,
           .data$databaseId,
           .data$timeAtRiskId,
           .data$methodId,
           .data$lookId,
           .data$events,
           .data$z)

  message("Computing critical values")
  # subset <- split(sampleSizePerLook, paste(sampleSizePerLook$exposureOutcomeId, sampleSizePerLook$databaseId, sampleSizePerLook$timeAtRiskId, sampleSizePerLook$methodId))[[100]]
  computeCv <- function(subset) {
    alpha <- alphaPerMethod %>%
      filter(.data$exposureOutcomeId == subset$exposureOutcomeId[1] &
               .data$databaseId == subset$databaseId[1] &
               .data$timeAtRiskId == subset$timeAtRiskId[1]) %>%
      pull(.data$alpha)
    sampleSizeUpperLimit <- max(subset$events, na.rm = TRUE)
    events <- subset$events
    if (length(events) > 1) {
      events[2:length(events)] <- events[2:length(events)] - events[1:(length(events)-1)]
      events <- events[events != 0]
    }
    if (length(events) == 0) {
      cv <- Inf
    } else {
      suppressMessages(
        cv <- EmpiricalCalibration::computeCvBinomial(groupSizes = events,
                                                      z = mean(subset$z),
                                                      minimumEvents = 1,
                                                      sampleSize = 1e7,
                                                      alpha = alpha)
      )
    }
    tibble(exposureOutcomeId = subset$exposureOutcomeId[1],
           databaseId = subset$databaseId[1],
           timeAtRiskId = subset$timeAtRiskId[1],
           methodId = subset$methodId[1],
           cv = cv) %>%
      return()
  }
  criticalValues <- map_dfr(split(sampleSizePerLook, paste(sampleSizePerLook$exposureOutcomeId,
                                                           sampleSizePerLook$databaseId,
                                                           sampleSizePerLook$timeAtRiskId,
                                                           sampleSizePerLook$methodId)),
                            computeCv)
  return(criticalValues)
}

computeCriticalValuesUsingExpected <- function(simulation, alphaPerMethod) {
  simulationSettings <- attr(simulation, "simulationSettings")
  exposureOutcomeSettings <- map_dfr(simulationSettings$exposureOutcomeSettings,
                                     function(x) return(tibble(z = x$nComparator / x$nTarget,
                                                               expectedEventsPerDay = x$backgroundRate * (x$nComparator + x$nTarget)))) %>%
    mutate(exposureOutcomeId = row_number())
  databaseSettings <- map_dfr(simulationSettings$databaseSettings,
                              function(x) return(tibble(sampleSizeMultiplier = x$sampleSizeMultiplier))) %>%
    mutate(databaseId = row_number())
  timeAtRiskSettings <- map_dfr(simulationSettings$timeAtRiskSettings,
                                function(x) return(tibble(start = x$start, end = x$end))) %>%
    mutate(timeAtRiskId  = row_number())
  values <- alphaPerMethod %>%
    inner_join(exposureOutcomeSettings, by = "exposureOutcomeId") %>%
    inner_join(databaseSettings, by = "databaseId") %>%
    inner_join(timeAtRiskSettings, by = "timeAtRiskId") %>%
    mutate(expectedEvents = .data$expectedEventsPerDay * .data$sampleSizeMultiplier * (.data$end - .data$start + 1))
  uniqueValues <- values %>%
    distinct(.data$expectedEvents, .data$z, .data$alpha)

  message("Computing critical values")
  # row <- split(uniqueValues, 1:nrow(uniqueValues))[[1]]
  computeCv <- function(row) {
    events <- round(1:simulationSettings$looks * row$expectedEvents)
    sampleSizeUpperLimit <- max(events, na.rm = TRUE)
    if (length(events) > 1) {
      events[2:length(events)] <- events[2:length(events)] - events[1:(length(events)-1)]
      events <- events[events != 0]
    }
    if (length(events) == 0) {
      cv <- Inf
    } else {
      suppressMessages(
        cv <- EmpiricalCalibration::computeCvBinomial(groupSizes = events,
                                                      z = row$z,
                                                      minimumEvents = 1,
                                                      sampleSize = 1e7,
                                                      alpha = row$alpha)
      )
    }
    row %>%
      mutate(cv = !!cv) %>%
      return()
  }
  uniqueCvs <- map_dfr(split(uniqueValues, 1:nrow(uniqueValues)), computeCv)

  criticalValues <- values %>%
    inner_join(uniqueCvs, by = c("alpha", "z", "expectedEvents")) %>%
    select(.data$exposureOutcomeId, .data$databaseId, .data$timeAtRiskId, .data$cv) %>%
    full_join(tibble(methodId = 1:length(simulationSettings$methodSettings)), by = character())
  return(criticalValues)
}
